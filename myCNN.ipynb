{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f0055e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import pathlib\n",
    "import splitfolders\n",
    "import torchvision.transforms as transforms # A module from the torchvision library that provides common image transformations, such as resizing, cropping, and normalization.\n",
    "import torchvision\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import copy # A module that provides functions for creating copies of objects, useful for avoiding unintended modifications to variables.\n",
    "from tqdm.notebook import trange, tqdm # These functions allow for the creation of progress bars to track the progress of loops or tasks.\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import seaborn as sns\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfd09f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72b5d1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New dataset path\n",
    "data_dir = r'C:\\Users\\Evans.Siaw\\Evans.Siaw\\evans\\evans\\pytorchProjects\\trainValSet'\n",
    "data_dir = pathlib.Path(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4076a7b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train', 'val']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(r'C:\\Users\\Evans.Siaw\\Evans.Siaw\\evans\\evans\\pytorchProjects\\trainValSet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f247541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transformation\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((256,256)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.485, 0.456, 0.406],std = [0.229, 0.224, 0.225])\n",
    "   ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16707e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=warn)\n",
       "    RandomHorizontalFlip(p=0.5)\n",
       "    RandomVerticalFlip(p=0.5)\n",
       "    RandomRotation(degrees=[-30.0, 30.0], interpolation=nearest, expand=False, fill=0)\n",
       "    ToTensor()\n",
       "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define an object of the custom dataset for the train and validation.\n",
    "train_set = torchvision.datasets.ImageFolder(data_dir.joinpath(\"train\"), transform=transform)\n",
    "train_set.transform\n",
    "val_set = torchvision.datasets.ImageFolder(data_dir.joinpath(\"val\"), transform=transform)\n",
    "val_set.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "835f272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and load train, validation\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = batch_size, shuffle = True, num_workers = 2)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size = batch_size, shuffle = True, num_workers = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d52282dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      "Shape of X : torch.Size([32, 3, 256, 256])\n",
      "Shape of y: torch.Size([32]) torch.int64\n",
      "\n",
      "Validation data:\n",
      "Shape of X : torch.Size([32, 3, 256, 256])\n",
      "Shape of y: torch.Size([32]) torch.int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key, value in {'Training data': train_loader, \"Validation data\": val_loader}.items():\n",
    "    for X, y in value:\n",
    "        print(f\"{key}:\")\n",
    "        print(f\"Shape of X : {X.shape}\")\n",
    "        print(f\"Shape of y: {y.shape} {y.dtype}\\n\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adac3ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findConv2dOutShape(hin,win,conv,pool=2):\n",
    "    # get conv arguments\n",
    "    kernel_size = conv.kernel_size\n",
    "    stride=conv.stride\n",
    "    padding=conv.padding\n",
    "    dilation=conv.dilation\n",
    "\n",
    "    hout=np.floor((hin+2*padding[0]-dilation[0]*(kernel_size[0]-1)-1)/stride[0]+1)\n",
    "    wout=np.floor((win+2*padding[1]-dilation[1]*(kernel_size[1]-1)-1)/stride[1]+1)\n",
    "\n",
    "    if pool:\n",
    "        hout/=pool\n",
    "        wout/=pool\n",
    "    return int(hout),int(wout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3f8f276e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainScanPlaneDetector(nn.Module):\n",
    "    def __init__(self, num_classes = 2):\n",
    "        super(BrainScanPlaneDetector, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv0 = nn.Conv2d(3, 64, kernel_size = 3)\n",
    "        h,w = findConv2dOutShape(256,256,self.conv0)\n",
    "        self.conv1 = nn.Conv2d(64, 128, kernel_size = 3)\n",
    "        h,w = findConv2dOutShape(h,w,self.conv1)\n",
    "        self.conv2 = nn.Conv2d(128, 256, kernel_size = 3)\n",
    "        h,w = findConv2dOutShape(h,w,self.conv2)\n",
    "        self.conv3 = nn.Conv2d(256, 512, kernel_size = 3)\n",
    "        h,w = findConv2dOutShape(h,w,self.conv3)\n",
    "            \n",
    "            \n",
    "        #Max pooling layers\n",
    "        self.maxPool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        \n",
    "        #Fully connected layers\n",
    "        self.num_flatten=h*w*512\n",
    "        self.fc1 = nn.Linear(self.num_flatten,512 )\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        \n",
    "        #Activation functions\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self,x): \n",
    "        # Convolutional layers with ReLU activation and max pooling\n",
    "        x = self.maxPool(self.relu(self.conv0(x)))\n",
    "        x = self.maxPool(self.relu(self.conv1(x)))\n",
    "        x = self.maxPool(self.relu(self.conv2(x)))\n",
    "        x = self.maxPool(self.relu(self.conv3(x)))\n",
    "        \n",
    "        # Flatten the output of the convolutional layers\n",
    "        x = x.view(-1, self.num_flatten)\n",
    "        \n",
    "        # Fully connected layers with dropout\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "abf9854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the BrainScanPlaneDetector model\n",
    "cnn_model = BrainScanPlaneDetector()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72cddc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e6f881c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 254, 254]           1,792\n",
      "              ReLU-2         [-1, 64, 254, 254]               0\n",
      "         MaxPool2d-3         [-1, 64, 127, 127]               0\n",
      "            Conv2d-4        [-1, 128, 125, 125]          73,856\n",
      "              ReLU-5        [-1, 128, 125, 125]               0\n",
      "         MaxPool2d-6          [-1, 128, 62, 62]               0\n",
      "            Conv2d-7          [-1, 256, 60, 60]         295,168\n",
      "              ReLU-8          [-1, 256, 60, 60]               0\n",
      "         MaxPool2d-9          [-1, 256, 30, 30]               0\n",
      "           Conv2d-10          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-11          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-12          [-1, 512, 14, 14]               0\n",
      "           Linear-13                  [-1, 512]      51,380,736\n",
      "             ReLU-14                  [-1, 512]               0\n",
      "          Dropout-15                  [-1, 512]               0\n",
      "           Linear-16                    [-1, 2]           1,026\n",
      "================================================================\n",
      "Total params: 52,932,738\n",
      "Trainable params: 52,932,738\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 127.87\n",
      "Params size (MB): 201.92\n",
      "Estimated Total Size (MB): 330.55\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Model Summary for CNN Model\n",
    "summary(cnn_model, input_size=(3,256, 256),device=device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1b0a3405",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Defining loss function\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4cccf5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Optimizer\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7e578f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ca79fa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, train_loader, valid_loader, loss_funtion, optimizer, num_epochs, save_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "      # initialize best loss to a large value\n",
    "    best_valid_loss = float('inf')\n",
    "    \n",
    "      # history of loss values in each epoch\n",
    "    loss_history={\"train\": [],\"val\": []}\n",
    "    # histroy of metric values in each epoch\n",
    "    accuracy_history={\"train\": [],\"val\": []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = loss_funtion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "            total_train += labels.size(0)\n",
    "        \n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "         # collect loss and accuracy for training\n",
    "        loss_history[\"train\"].append(train_loss)\n",
    "        accuracy_history[\"train\"].append(train_accuracy)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        correct_valid = 0\n",
    "        total_valid = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in valid_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = loss_funtion(outputs, labels)\n",
    "                \n",
    "                valid_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_valid += (predicted == labels).sum().item()\n",
    "                total_valid += labels.size(0)\n",
    "        \n",
    "        valid_accuracy = 100 * correct_valid / total_valid\n",
    "        valid_loss /=  len(valid_loader)\n",
    "        \n",
    "         # collect loss and accuracy for validation \n",
    "        loss_history[\"val\"].append(valid_loss)\n",
    "        accuracy_history[\"val\"].append(valid_accuracy)\n",
    "        \n",
    "        # Save the model if validation loss improves\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "        \n",
    "        # Print training and validation statistics\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, '\n",
    "              f'Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_accuracy:.2f}%')\n",
    "        \n",
    "    return loss_history, accuracy_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "94e2c8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 0.1178, Train Acc: 96.69%, Valid Loss: 0.0394, Valid Acc: 98.87%\n",
      "Epoch [2/10], Train Loss: 0.0548, Train Acc: 98.25%, Valid Loss: 0.0337, Valid Acc: 99.12%\n",
      "Epoch [3/10], Train Loss: 0.0484, Train Acc: 98.44%, Valid Loss: 0.1730, Valid Acc: 93.12%\n",
      "Epoch [4/10], Train Loss: 0.0619, Train Acc: 98.28%, Valid Loss: 0.0332, Valid Acc: 99.25%\n",
      "Epoch [5/10], Train Loss: 0.1706, Train Acc: 97.25%, Valid Loss: 4.7785, Valid Acc: 55.32%\n",
      "Epoch [6/10], Train Loss: 0.2190, Train Acc: 96.19%, Valid Loss: 0.0683, Valid Acc: 98.87%\n",
      "Epoch [7/10], Train Loss: 0.0558, Train Acc: 98.22%, Valid Loss: 0.0447, Valid Acc: 98.87%\n",
      "Epoch [8/10], Train Loss: 0.0573, Train Acc: 98.34%, Valid Loss: 0.0448, Valid Acc: 98.75%\n",
      "Epoch [9/10], Train Loss: 0.0388, Train Acc: 98.84%, Valid Loss: 0.0355, Valid Acc: 99.37%\n",
      "Epoch [10/10], Train Loss: 0.0676, Train Acc: 97.84%, Valid Loss: 0.0479, Valid Acc: 99.00%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "save_path='model.pth'\n",
    "loss_hist,accuracy_hist = train_and_evaluate_model(model, train_loader,val_loader, loss_func, optimizer, 10, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c8bab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "20a65c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = model.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2f11c896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': [0.08147877355106176], 'val': [0.07840124290436506]}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = {\n",
    "    \"loss_hist['train']\": loss_hist[\"train\"],\n",
    "    \"loss_hist['val']\": loss_hist[\"val\"],\n",
    "    \"accuracy_hist['train']\": accuracy_hist[\"train\"],\n",
    "    \"accuracy_hist['val']\": accuracy_hist[\"val\"]\n",
    "}\n",
    "\n",
    "df = DataFrame(data)\n",
    "df.csv(\"modelResult.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "81979496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': [97.5625], 'val': [97.74718397997496]}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "35076803",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [81]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Convergence History Plot\u001b[39;00m\n\u001b[0;32m      3\u001b[0m fig,ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m----> 5\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlineplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_hist\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43max\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss_hist[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m sns\u001b[38;5;241m.\u001b[39mlineplot(x\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,num_epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)],y\u001b[38;5;241m=\u001b[39mloss_hist[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m],ax\u001b[38;5;241m=\u001b[39max[\u001b[38;5;241m0\u001b[39m],label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_hist[\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m sns\u001b[38;5;241m.\u001b[39mlineplot(x\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,num_epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)],y\u001b[38;5;241m=\u001b[39maccuracy_hist[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],ax\u001b[38;5;241m=\u001b[39max[\u001b[38;5;241m1\u001b[39m],label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy_hist[\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:46\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPass the following variable\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m as \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124mkeyword arg\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrom version 0.12, the only valid positional argument \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     )\n\u001b[0;32m     45\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mupdate({k: arg \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args)})\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\seaborn\\relational.py:692\u001b[0m, in \u001b[0;36mlineplot\u001b[1;34m(x, y, hue, size, style, data, palette, hue_order, hue_norm, sizes, size_order, size_norm, dashes, markers, style_order, units, estimator, ci, n_boot, seed, sort, err_style, err_kws, legend, ax, **kwargs)\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[38;5;129m@_deprecate_positional_args\u001b[39m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlineplot\u001b[39m(\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    688\u001b[0m     legend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, ax\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    689\u001b[0m ):\n\u001b[0;32m    691\u001b[0m     variables \u001b[38;5;241m=\u001b[39m _LinePlotter\u001b[38;5;241m.\u001b[39mget_semantics(\u001b[38;5;28mlocals\u001b[39m())\n\u001b[1;32m--> 692\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[43m_LinePlotter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mci\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mci\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_boot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_boot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merr_style\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr_style\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merr_kws\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr_kws\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlegend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlegend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    698\u001b[0m     p\u001b[38;5;241m.\u001b[39mmap_hue(palette\u001b[38;5;241m=\u001b[39mpalette, order\u001b[38;5;241m=\u001b[39mhue_order, norm\u001b[38;5;241m=\u001b[39mhue_norm)\n\u001b[0;32m    699\u001b[0m     p\u001b[38;5;241m.\u001b[39mmap_size(sizes\u001b[38;5;241m=\u001b[39msizes, order\u001b[38;5;241m=\u001b[39msize_order, norm\u001b[38;5;241m=\u001b[39msize_norm)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\seaborn\\relational.py:367\u001b[0m, in \u001b[0;36m_LinePlotter.__init__\u001b[1;34m(self, data, variables, estimator, ci, n_boot, seed, sort, err_style, err_kws, legend)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m    355\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, variables\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;66;03m# the kind of plot to draw, but for the time being we need to set\u001b[39;00m\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;66;03m# this information so the SizeMapping can use it\u001b[39;00m\n\u001b[0;32m    363\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_size_range \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    364\u001b[0m         np\u001b[38;5;241m.\u001b[39mr_[\u001b[38;5;241m.5\u001b[39m, \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m*\u001b[39m mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlines.linewidth\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    365\u001b[0m     )\n\u001b[1;32m--> 367\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator \u001b[38;5;241m=\u001b[39m estimator\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mci \u001b[38;5;241m=\u001b[39m ci\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\seaborn\\_core.py:605\u001b[0m, in \u001b[0;36mVectorPlotter.__init__\u001b[1;34m(self, data, variables)\u001b[0m\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, variables\u001b[38;5;241m=\u001b[39m{}):\n\u001b[1;32m--> 605\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    607\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m var, \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_semantic_mappings\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    608\u001b[0m \n\u001b[0;32m    609\u001b[0m         \u001b[38;5;66;03m# Create the mapping function\u001b[39;00m\n\u001b[0;32m    610\u001b[0m         map_func \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmap, plotter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\seaborn\\_core.py:668\u001b[0m, in \u001b[0;36mVectorPlotter.assign_variables\u001b[1;34m(self, data, variables)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    667\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlong\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 668\u001b[0m     plot_data, variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_variables_longform(\n\u001b[0;32m    669\u001b[0m         data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mvariables,\n\u001b[0;32m    670\u001b[0m     )\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplot_data \u001b[38;5;241m=\u001b[39m plot_data\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables \u001b[38;5;241m=\u001b[39m variables\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\seaborn\\_core.py:927\u001b[0m, in \u001b[0;36mVectorPlotter._assign_variables_longform\u001b[1;34m(self, data, **kwargs)\u001b[0m\n\u001b[0;32m    923\u001b[0m         variables[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(val, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# Construct a tidy plot DataFrame. This will convert a number of\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;66;03m# types automatically, aligning on index in case of pandas objects\u001b[39;00m\n\u001b[1;32m--> 927\u001b[0m plot_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplot_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;66;03m# Reduce the variables dictionary to fields with valid data\u001b[39;00m\n\u001b[0;32m    930\u001b[0m variables \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    931\u001b[0m     var: name\n\u001b[0;32m    932\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m var, name \u001b[38;5;129;01min\u001b[39;00m variables\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m plot_data[var]\u001b[38;5;241m.\u001b[39mnotnull()\u001b[38;5;241m.\u001b[39many()\n\u001b[0;32m    934\u001b[0m }\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:636\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    630\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    631\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    632\u001b[0m     )\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    635\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 636\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmrecords\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmrecords\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:502\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    494\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    495\u001b[0m         x\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[0;32m    497\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m x\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[0;32m    499\u001b[0m     ]\n\u001b[0;32m    500\u001b[0m     \u001b[38;5;66;03m# TODO: can we get rid of the dt64tz special case above?\u001b[39;00m\n\u001b[1;32m--> 502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:120\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 120\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:674\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    672\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 674\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    678\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    679\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convergence History Plot\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(12,5))\n",
    "\n",
    "sns.lineplot(x=[*range(1,num_epochs+1)],y=loss_hist[\"train\"],ax=ax[0],label='loss_hist[\"train\"]')\n",
    "sns.lineplot(x=[*range(1,num_epochs+1)],y=loss_hist[\"val\"],ax=ax[0],label='loss_hist[\"val\"]')\n",
    "sns.lineplot(x=[*range(1,num_epochs+1)],y=accuracy_hist[\"train\"],ax=ax[1],label='accuracy_hist[\"train\"]')\n",
    "sns.lineplot(x=[*range(1,num_epochs+1)],y=accuracy_hist[\"val\"],ax=ax[1],label='accuracy_hist[\"val\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4c21e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e7359f86",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'_IncompatibleKeys' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [76]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y_true, y_pred\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# check confusion matrix for error analysis\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mTrue_and_Pred\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcnn_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_true, y_pred), \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     23\u001b[0m cm \u001b[38;5;241m=\u001b[39m confusion_matrix(y_true, y_pred)\n",
      "Input \u001b[1;32mIn [76]\u001b[0m, in \u001b[0;36mTrue_and_Pred\u001b[1;34m(val_loader, model)\u001b[0m\n\u001b[0;32m      7\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      8\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m----> 9\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m _, pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     11\u001b[0m pred \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[1;31mTypeError\u001b[0m: '_IncompatibleKeys' object is not callable"
     ]
    }
   ],
   "source": [
    "# define function For Classification Report\n",
    "def True_and_Pred(val_loader, model):\n",
    "    i = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for images, labels in val_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.numpy()\n",
    "        outputs = model(images)\n",
    "        _, pred = torch.max(outputs.data, 1)\n",
    "        pred = pred.detach().cpu().numpy()\n",
    "\n",
    "        y_true = np.append(y_true, labels)\n",
    "        y_pred = np.append(y_pred, pred)\n",
    "\n",
    "    return y_true, y_pred\n",
    "\n",
    "\n",
    "# check confusion matrix for error analysis\n",
    "y_true, y_pred = True_and_Pred(val_loader, cnn_model)\n",
    "\n",
    "print(classification_report(y_true, y_pred), '\\n\\n')\n",
    "cm = confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba007ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLA_label = {\n",
    "    0 : 'Axial',\n",
    "    1 : 'Sagital'\n",
    "}\n",
    "\n",
    "# Confusion Matrix Plotting Function\n",
    "def show_confusion_matrix(cm, CLA_label, title='Confusion matrix', cmap=plt.cm.YlGnBu):\n",
    "\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.grid(False)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(CLA_label))\n",
    "\n",
    "    plt.xticks(tick_marks, [f\"{value}={key}\" for key , value in CLA_label.items()], rotation=45)\n",
    "    plt.yticks(tick_marks, [f\"{value}={key}\" for key , value in CLA_label.items()])\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, f\"{cm[i,j]}\\n{cm[i,j]/np.sum(cm)*100:.2f}%\", horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_confusion_matrix(cm, CLA_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1119590d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
